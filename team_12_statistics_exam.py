# -*- coding: utf-8 -*-
"""Team 12 Statistics_Exam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UgraIEXUUeWg3H5QzV_Y_ru9YmwQ_ff_
"""

from google.colab import drive
drive.mount('/content/drive/')

#Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""# Data Preparation"""

#reading Dataset
df = pd.read_csv(r"/content/drive/MyDrive/Statistic_Project/application_data (1).csv")

df.head()

df.shape
#WE CAN OBSERVE THERE ARE 307511 rows and 122 columns

#Finding the information of the data
df.info('all')
#Data types of each and every column is know over here
#Heere there are int type as well as object and float type

#Finding Statistical information about the data
df.describe()

"""Observation:

Mean of Target is 0.080
Standerd deviation of Target is 0.2724

Similarly,
  
AMT_INCOME_TOTAL has mean of 1.687979e+05
Standard Deviation is 2.371231e+05


Hence in the above table, we can observe, major statistical features of the dataset

# Checking for Null values
"""

total = df.isnull().sum()
print(total.to_string())
#Here we can see there are AMT_ANNUITY has 12 null values which is the lowest 
# similarly NAME_TYPE_SUITE are 1292 Null values
# OWN_CAR_AGE has 202929 Null values
#And so on

"""# Handeling Null Values

Identify the missing data and use appropriate methods to deal with it. (Remove columns/or replace it with an appropriate value)
"""

#Observation:
# Here AMT_ANNUITY, AMT_GOODS_PRICE, CNT_FAM_MEMBERS has the least null values, thus we can fill it with median
median_amt = df['AMT_ANNUITY'].median()
df['AMT_ANNUITY'].fillna(median_amt, inplace=True)

df['AMT_ANNUITY'].isnull().sum()

median_amt = df['AMT_GOODS_PRICE'].median()
df['AMT_GOODS_PRICE'].fillna(median_amt, inplace=True)

median_amt = df['CNT_FAM_MEMBERS'].median()
df['CNT_FAM_MEMBERS'].fillna(median_amt, inplace=True)

median_amt = df['DAYS_LAST_PHONE_CHANGE'].median()
df['DAYS_LAST_PHONE_CHANGE'].fillna(median_amt, inplace=True)

#creating a function to find null values for the dataframe
def null_values(df):
    return round((df.isnull().sum()*100/len(df)).sort_values(ascending = False),2)

#Keeping out threshold as 50%,
#creating a variable null_vals for storing null columns having missing values more than 50%
null_vals = null_values(df)[null_values(df)>50]

null_vals

# Observation:
# These are the columns having more that 50% of null value decided to delete it.

#Craeating the backup to, retrive original dataset incase of emergency
df_backup = df.copy()

#Droping
df.drop(columns = null_vals.index, inplace = True)
#Dropping the columns having more than 50% of Null values

df.shape

#Now checking null values more than
null_col_15 = null_values(df)[null_values(df)>15]

null_col_15

""" from the columns dictionary we can conclude that only 'OCCUPATION_TYPE',
'EXT_SOURCE_3 looks relevant to TARGET column.
 thus dropping all other columns except 'OCCUPATION_TYPE','EXT_SOURCE_3
"""

# from the columns dictionary we can conclude that only 'OCCUPATION_TYPE',
#  'EXT_SOURCE_3 looks relevant to TARGET column.
#  thus dropping all other columns except 'OCCUPATION_TYPE','EXT_SOURCE_3
null_col_15.drop(["OCCUPATION_TYPE","EXT_SOURCE_3"], inplace = True)

null_col_15

df.info('all')

df.drop(null_col_15.index,axis=1, inplace = True)

df.shape



total = df.isnull().sum()
print(total.to_string())

null_vals_greater_than_15 = null_values(df)[null_values(df)>15]

# now we will deal with null values more than 15% 
appl_data = df.copy()
null_col_15 = null_values(appl_data)[null_values(appl_data)>15]

#Checking Correlation of EXT_SOURCE_3 , EXT_SOURCE_2.
df.corr()

import seaborn as sns
# Starting with EXT_SOURCE_3 , EXT_SOURCE_2. As they have normalised values, now we will understand the relation between these columns with TARGET column using a heatmap
irrev = ["EXT_SOURCE_3","EXT_SOURCE_2"]  # putting irrevlent columns in varibale "irrev"
plt.figure(figsize= [10,7])

sns.heatmap(appl_data[irrev+["TARGET"]].corr(), cmap="Reds",annot=True)

plt.title("Correlation between EXT_SOURCE_3, EXT_SOURCE_2, TARGET", fontdict={"fontsize":20}, pad=25)
plt.show()

#As there doesn't seem to be correlatiob=n between the EXT_SOURCE_3 , EXT_SOURCE_2 and the target thus dropping these columns too
df.drop(irrev, axis=1, inplace=True)

df.shape

print(null_values(df).to_string())

#Occupation type being important factor, thus instead of dropping the rows of occupation type, we have decided to fill it with a common value
df['OCCUPATION_TYPE'].fillna('Not_Known', inplace=True)

print(null_values(df).to_string())

"""Succsfully Handeled Null Values"""

#Putting all the flag columns in a single list
flag_columns = [col for col in appl_data.columns if "FLAG" in col]

len(flag_columns)
#Thus the amount of flag columns are 28

"""Checking for correlation

Claims made:
1. EXT_SOURCE_3 , EXT_SOURCE_2 has Less correlation with target values
2. Flags have correlation with Target values
"""

#Checking for claims
# Starting with EXT_SOURCE_3 , EXT_SOURCE_2. As they have normalised values, now we will understand the relation between these columns with TARGET column using a heatmap
irrev = ["EXT_SOURCE_3","EXT_SOURCE_2"]  # putting irrevlent columns in varibale "irrev"
plt.figure(figsize= [50,17])

sns.heatmap(appl_data[flag_columns+["TARGET"]].corr(), cmap="Reds",annot=True)

plt.title("Correlation between Flags and TARGET", fontdict={"fontsize":30}, pad=25)
plt.show()

flag_df = appl_data[flag_columns+["TARGET"]]

# replacing "0" as repayer and "1" as defaulter for TARGET column

flag_df["TARGET"] = flag_df["TARGET"].replace({1:"Defaulter", 0:"Repayer"})

flag_df["TARGET"]

for i in flag_df:
  if i!="Target":
      flag_df[i] = flag_df[i].replace({1: 'Y', 0: 'N'})

flag_df.head()
#Succesfully converted the 0 and 1 to Y and N

"""# **Performing BIVARIATE Analysis**
1. FLAG_WORK_PHONE vs TARGET
2. FLAG_DOCUMENT_3 vs TARGET
3. FLAG_EMP_PHONE vs TARGET
"""

sns.countplot(flag_df['FLAG_WORK_PHONE'], hue= flag_df['TARGET'])
#We can see that People dosen't have a work phone seems to be able to pay as compared ti the people having workphone

sns.countplot(flag_df['FLAG_DOCUMENT_3'], hue= flag_df['TARGET'])

##Conclusion is that There are more Repayers with the people having Document_3

sns.countplot(flag_df['FLAG_EMP_PHONE'], hue= flag_df['TARGET'])
##We can colclude that Repayers aree more where people have Phone

# Here FLAG_DOCUMENT_3 FLAG_WORK_PHONE FLAG_EMP_PHONE SEEMS TO CORRELATE WITH THE DATA THUS KEEPING IT REMOVING OTHERS
flag_df.drop(['FLAG_DOCUMENT_3', 'FLAG_WORK_PHONE', 'FLAG_EMP_PHONE', 'TARGET'], axis=1, inplace=True)

flag_df.shape

# dropping irrelevent flag terms from the data
df.drop(flag_df.columns, axis=1, inplace=True)

"""# Univariate Anaysis -> Looking into Occupation"""

# Finding percentage of people belonging to each occupation

plt.figure(figsize = [12,7])
(df["OCCUPATION_TYPE"].value_counts()).plot.bar(color= "Red",width = .8)
plt.title("Occupations Percentage each", fontdict={"fontsize":20}, pad =20)
plt.show()

#Conclusion:
# As we can concluded, majority of people's occupation is unknown, dropping such rows would have been resulted in shortage of dataset,
#  thus a common value is fair enough to be replaced
# This graph represent the amount of people in each profession, here we can see that amount of people working as a Labours are
# # More as compared to others

df.shape

df['REGION_RATING_CLIENT'].value_counts()

"""## Looking for Outliers"""

df.describe()

"""**On the observation of dataframe, we can see that, some columns have difference higher between max and 75 percentile which seems to be as a presence of outliers**"""

outlier_col = ["CNT_CHILDREN","AMT_INCOME_TOTAL", "AMT_CREDIT", "AMT_ANNUITY", "AMT_GOODS_PRICE",
               "DAYS_BIRTH", "DAYS_EMPLOYED", "DAYS_REGISTRATION"]

import itertools
plt.figure(figsize=[15,25])
for i,j in itertools.zip_longest(outlier_col, range(len(outlier_col))):
    plt.subplot(4,2,j+1)
    sns.boxplot(y = appl_data[i], orient = "h", color = "yellow")
    plt.xlabel("")
    plt.ylabel("")
    plt.title(i)

# Observation in Outliers
# AMT_ANNUITY, AMT_CREDIT, AMT_GOODS_PRICE,CNT_CHILDREN have some number of outliers.
# AMT_INCOME_TOTAL has huge number of outliers which indicate that few of the loan applicants have high income when compared to the others.
# DAYS_BIRTH shows no outliers so the data is reliable
#DAYS_EMPLOYEED HAS AN OUTLIER AT 350000 I.E 350000/365 = 958.9041095890411 years which is not possible, Hence it can be a chance variation.

#Data imbalance

"""# Data imbalance"""

df['AGE'] = df['DAYS_BIRTH']/365

plt.figure(figsize = [12,7])
sns.distplot(df['AGE'])
#Checking for variation in particular columns

#The data seems to be distributed with no major peaks.

plt.figure(figsize = [12,7])
sns.distplot(df['AMT_ANNUITY'])

# After plotting the displot we can conclude that AMT_ANNUITY is right skwed graph which means it is positive

plt.figure(figsize = [12,7])
sns.distplot(df['DAYS_REGISTRATION'])

## After plotting the displot we can conclude that DAYS_REGISTRATION is left skwed graph which means it is Negative

print('#'*300)



"""# Dataset -> Previous Application"""

#importing second dataset
#name = previous_application.csv
df2 = pd.read_csv(r"/content/drive/MyDrive/Statistic_Project/previous_application.csv")

df2.head()

df2.shape
#Observation:
# Dataset has 1670214 and 37 columns

# Statistical information about data
df2.describe()

print(null_values(df2))
#Observation:
#Some columns has 99.64 % of Null values

#To verify the claim
df2['RATE_INTEREST_PRIVILEGED'].isnull().sum()

#creating a variable p_null_col_50 for storing null columns having missing values more than 50%
p_null_col_50 = null_values(df2)[null_values(df2)>50]

p_null_col_50
#There are only 4 columns where the null values are more than 50%

#dropping null columns having missing values more than 50%
df2.drop(columns = p_null_col_50.index, inplace = True)

#creating a variable p_null_col_50 for storing null columns having missing values more than 15%
p_null_col_15 = null_values(df2)[null_values(df2)>15]

p_null_col_15
#These are the rows that have more than 15% null values



"""# Checking for the null values of
 AMT_GOODS_PRICE              

AMT_ANNUITY  
          
CNT_PAYMENT
"""

df2[p_null_col_15.index]

df2.shape

# Listing down columns which are not required
not_required_df2 = ['WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START','FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY']

df2.drop(not_required_df2,axis =1, inplace = True)

df2.shape

df2.columns

df2['AMT_ANNUITY'].isnull().sum()

plt.figure(figsize = [12,7])
sns.distplot(df2['AMT_ANNUITY'])
plt.show()

# There is a single peak at the left side of the distribution and it indicates the presence of outliers and 
# hence imputing with mean would not be the right approach and hence imputing with median.

median_amt_aanuity = df2['AMT_ANNUITY'].median()

#replacing null values with median
df2['AMT_ANNUITY'].fillna(df2['AMT_ANNUITY'].median(),inplace = True)

plt.figure(figsize = [12,7])
sns.distplot(df2['AMT_ANNUITY'])
plt.show()

#After replacing the null values with median values we plotted the distplot for AMT_ANNUITY with density and variance is found out be less.

df2['AMT_ANNUITY'].isnull().sum()

plt.figure(figsize = [12,7])
sns.distplot(df2['AMT_GOODS_PRICE'])
plt.show()

# Creating new dataframe for "AMT_GOODS_PRICE" with columns imputed with mode, median and mean

statsDF = pd.DataFrame() 
statsDF['AMT_GOODS_PRICE_mode'] = df2['AMT_GOODS_PRICE'].fillna(df2['AMT_GOODS_PRICE'].mode()[0])
statsDF['AMT_GOODS_PRICE_median'] = df2['AMT_GOODS_PRICE'].fillna(df2['AMT_GOODS_PRICE'].median())
statsDF['AMT_GOODS_PRICE_mean'] = df2['AMT_GOODS_PRICE'].fillna(df2['AMT_GOODS_PRICE'].mean())

cols = ['AMT_GOODS_PRICE_mode', 'AMT_GOODS_PRICE_median','AMT_GOODS_PRICE_mean']

plt.figure(figsize=(18,10))
plt.suptitle('Distribution of Original data vs imputed data')
plt.subplot(221)
sns.distplot(df2['AMT_GOODS_PRICE'][pd.notnull(df2['AMT_GOODS_PRICE'])]);
for i in enumerate(cols): 
    plt.subplot(2,2,i[0]+2)
    sns.distplot(statsDF[i[1]])

"""The original distribution is closer with the distribution of data imputed with mode in this case, thus will impute mode for missing values

"""

# Imputing null values with mode
df2['AMT_GOODS_PRICE'].fillna(df2['AMT_GOODS_PRICE'].mode()[0], inplace=True)

df2['AMT_GOODS_PRICE'].isnull().sum()

"""Successfully handeled all the null values in AMT_GOODS_PRICE column

# Finding Outliers
"""

df2.describe()

claimed_outliers_column = ['AMT_ANNUITY','AMT_APPLICATION','AMT_CREDIT','AMT_GOODS_PRICE',
                 'SELLERPLACE_AREA','DAYS_DECISION','CNT_PAYMENT', 'DAYS_LAST_DUE_1ST_VERSION']

plt.figure(figsize=[15,25])
for i,j in itertools.zip_longest(claimed_outliers_column, range(len(claimed_outliers_column))):
    plt.subplot(4,2,j+1)
    sns.boxplot(y = df2[i], orient = "h", color = "orange")
    plt.xlabel("")
    plt.ylabel("")
    plt.title(i)

sns.boxplot(df2['DAYS_FIRST_DUE'], orient = "h", color = "orange")

"""can be seen that in previous application data

AMT_ANNUITY, AMT_APPLICATION, AMT_CREDIT, AMT_GOODS_PRICE, SELLERPLACE_AREA have huge number of outliers.
CNT_PAYMENT has few outlier values.
DAYS_DECISION has little number of outliers indicating that these previous applications decisions were taken long back.

# Imbalance Data
"""

#Plotting a barplot between reprayer vs defaulter
plt.figure(figsize= [14,5])
sns.barplot(y=["Repayer","Defaulter"], x = df["TARGET"].value_counts(), palette = ["blue","r"],orient="h")
plt.ylabel("Loan Repayment Status",fontdict = {"fontsize":15})
plt.xlabel("Count",fontdict = {"fontsize":15})
plt.title("Imbalance Plotting (Repayer Vs Defaulter)", fontdict = {"fontsize":25}, pad = 20)
plt.show()

# Analysis

df.groupby('NAME_INCOME_TYPE')['AMT_INCOME_TOTAL'].describe()

"""It can be seen that Businessman income is the highest and the estimated range with default 95% confidence level seem to indicate that the income of a Businessman could be in the range of slightly close to 4 lakhs and slightly above 10 lakhs

"""

cols_for_correlation = ['NAME_CONTRACT_TYPE', 'CODE_GENDER',
       'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',
       'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',
       'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',
       'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED',
       'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'FLAG_EMP_PHONE',
       'FLAG_WORK_PHONE', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS',
       'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY',
       'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START',
       'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',
       'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',
       'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY',
       'ORGANIZATION_TYPE', 'OBS_30_CNT_SOCIAL_CIRCLE',
       'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',
       'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_3',
       'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',
       'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON',
       'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']

Repayer_df = appl_data.loc[appl_data['TARGET']==0, cols_for_correlation]

"""Find the top 10 correlation for the Client with payment difficulties and all other cases (Target variable). Note that you have to find the top correlation by segmenting the data frame w.r.t to the target variable and then find the top correlation for each of the segmented data and find if any insight is there.  Say, there are 5+1(target) variables in a dataset: Var1, Var2, Var3, Var4, Var5, Target. And if you have to find the top 3 correlation, it can be: Var1 & Var2, Var2 & Var3, Var1 & Var3. Target variable will not feature in this correlation as it is a categorical variable and not a continuous variable which is increasing or decreasing.  

"""

# Getting  top 10 correlation for the Repayers dataframe
corr_repayer = Repayer_df.corr()
corr_df_repayer = corr_repayer.where(np.triu(np.ones(corr_repayer.shape),k=1).astype(np.bool)).unstack().reset_index()
corr_df_repayer.columns =['VAR1','VAR2','Correlation']
corr_df_repayer.dropna(subset = ["Correlation"], inplace = True)
corr_df_repayer["Correlation"]=corr_df_repayer["Correlation"].abs() 
corr_df_repayer.sort_values(by='Correlation', ascending=False, inplace=True) 
corr_df_repayer.head(10)

#plotting heatmap to see linear correlation amoung Repayers 

fig = plt.figure(figsize=(20,15))
ax = sns.heatmap(Repayer_df.corr(),annot=True,linewidth =1)

# Plotting the numerical columns related to amount as distribution plot to see density
amount = appl_data[['AMT_ANNUITY', 'AMT_GOODS_PRICE']]

fig = plt.figure(figsize=(16,12))

for i in enumerate(amount):
    plt.subplot(2,2,i[0]+1)
    # sns.distplot(Defaulter_df[i[1]], hist=False,label ="Defaulter")
    sns.distplot(Repayer_df[i[1]], hist=False, label ="Repayer")
    plt.title(i[1], fontdict={'fontsize' : 15, 'fontweight' : 5})
    plt.legend()

plt.show()

"""Most no of loans are given for goods price below 10 lakhs
Most people pay annuity below 50K for the credit loan

# **Machine Models can be used are:**

On basis of the database, we conclude that, Supervised Machine Learning Models can 

1. Binary Logistic Linear Rigression

Details: 

  As the output of the model is in 0 and 1 as repayer and Defaulter, Binary Regression can predict the output as either customer can be Repayer or defaulter based on its data given.

2. Descision Tree

  Details:

    Descision tree Can predict the output, being internal nodes representing   whole dataset and leaf nodes being the predictions.

3. Random Forest

Details:

  Combination of various Decision trees can be used for prediction using Random Forest, However the the computation time required to train such Random Forest Model is greater as compared to Decision tree but the prediction and the accuracy will be greater than descision tree.

4. K-NN Classifier

  Details:

  K-NN is the K-Nearest Neighbour Classifier, keep track of similarity in the dataset and classifies new data by comparing it with already present data.
"""

#As we can see, Counts of females is more than as compred to males in repayers.
sns.countplot(Repayer_df['CODE_GENDER'])

#As we can see, Counts of Married is more than as compred to others in repayers.
sns.countplot(Repayer_df['NAME_FAMILY_STATUS'])

"""# **Conclusion**


1. Bank Should Focus on the People having FLAG_DOCUMENT_3, they seems to be the majority in Repayers as compared to others.

2. The percentage of IT professions applying for loans is less than as compared to other professions
Thus, Bank can focus on other professions rather thaM IT sectors

3. As we can see, Counts of females is more than as compred to males in repayers

4. Bank can focus more on married peoples because Count of Married people is more than as compred to others in repayers.
"""